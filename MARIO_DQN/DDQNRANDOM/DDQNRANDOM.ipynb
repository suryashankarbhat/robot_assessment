{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dda1aecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym in c:\\programdata\\anaconda3\\lib\\site-packages (0.23.1)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from gym) (0.0.6)\n",
      "Requirement already satisfied: importlib-metadata>=4.10.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from gym) (4.11.3)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from gym) (1.6.0)\n",
      "Requirement already satisfied: numpy>=1.18.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from gym) (1.20.1)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from importlib-metadata>=4.10.0->gym) (3.4.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68a2c0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym_super_mario_bros in c:\\programdata\\anaconda3\\lib\\site-packages (7.3.2)\n",
      "Requirement already satisfied: nes-py>=8.1.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from gym_super_mario_bros) (8.1.8)\n",
      "Requirement already satisfied: gym>=0.17.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from nes-py>=8.1.2->gym_super_mario_bros) (0.23.1)\n",
      "Requirement already satisfied: tqdm>=4.48.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from nes-py>=8.1.2->gym_super_mario_bros) (4.59.0)\n",
      "Requirement already satisfied: pyglet<=1.5.11,>=1.4.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from nes-py>=8.1.2->gym_super_mario_bros) (1.5.11)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from nes-py>=8.1.2->gym_super_mario_bros) (1.20.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.10.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from gym>=0.17.2->nes-py>=8.1.2->gym_super_mario_bros) (4.11.3)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from gym>=0.17.2->nes-py>=8.1.2->gym_super_mario_bros) (1.6.0)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from gym>=0.17.2->nes-py>=8.1.2->gym_super_mario_bros) (0.0.6)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from importlib-metadata>=4.10.0->gym>=0.17.2->nes-py>=8.1.2->gym_super_mario_bros) (3.4.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gym_super_mario_bros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4112184",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gym\\envs\\registration.py:505: UserWarning: \u001b[33mWARN: The environment SuperMarioBros-1-1-v0 is out of date. You should consider upgrading to version `v3` with the environment ID `SuperMarioBros-1-1-v3`.\u001b[0m\n",
      "  logger.warn(\n",
      "<ipython-input-3-91fb2e089e18>:119: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
      "  torch.tensor([action]), torch.tensor([reward]), torch.tensor([done])))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10 - Step 2635 - Epsilon 0.01 - Mean Reward 1717.7\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_10.pth'\n",
      "Episode 20 - Step 5576 - Epsilon 0.01 - Mean Reward 1359.4\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_20.pth'\n",
      "Episode 30 - Step 7412 - Epsilon 0.01 - Mean Reward 1022.2\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_30.pth'\n",
      "Episode 40 - Step 9975 - Epsilon 0.01 - Mean Reward 1493.5\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_40.pth'\n",
      "Episode 50 - Step 13028 - Epsilon 0.01 - Mean Reward 1540.8\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_50.pth'\n",
      "Episode 60 - Step 16615 - Epsilon 0.01 - Mean Reward 1729.9\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_60.pth'\n",
      "Episode 70 - Step 19887 - Epsilon 0.01 - Mean Reward 1695.5\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_70.pth'\n",
      "Episode 80 - Step 23759 - Epsilon 0.01 - Mean Reward 1615.6\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_80.pth'\n",
      "Episode 90 - Step 26755 - Epsilon 0.01 - Mean Reward 1843.0\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_90.pth'\n",
      "Episode 100 - Step 29944 - Epsilon 0.01 - Mean Reward 1825.6\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_100.pth'\n",
      "Episode 110 - Step 33184 - Epsilon 0.01 - Mean Reward 2118.6\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_110.pth'\n",
      "Episode 120 - Step 36183 - Epsilon 0.01 - Mean Reward 1700.0\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_120.pth'\n",
      "Episode 130 - Step 40748 - Epsilon 0.01 - Mean Reward 1855.3\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_130.pth'\n",
      "Episode 140 - Step 45002 - Epsilon 0.01 - Mean Reward 2116.1\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_140.pth'\n",
      "Episode 150 - Step 49784 - Epsilon 0.01 - Mean Reward 2385.7\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_150.pth'\n",
      "Episode 160 - Step 53351 - Epsilon 0.01 - Mean Reward 2087.7\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_160.pth'\n",
      "Episode 170 - Step 58183 - Epsilon 0.01 - Mean Reward 1828.8\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_170.pth'\n",
      "Episode 180 - Step 63337 - Epsilon 0.01 - Mean Reward 2297.8\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_180.pth'\n",
      "Episode 190 - Step 67315 - Epsilon 0.01 - Mean Reward 2345.8\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_190.pth'\n",
      "Episode 200 - Step 71338 - Epsilon 0.01 - Mean Reward 2119.6\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_200.pth'\n",
      "Episode 210 - Step 74813 - Epsilon 0.01 - Mean Reward 1920.5\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_210.pth'\n",
      "Episode 220 - Step 80225 - Epsilon 0.01 - Mean Reward 2309.0\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_220.pth'\n",
      "Episode 230 - Step 83364 - Epsilon 0.01 - Mean Reward 1776.7\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_230.pth'\n",
      "Episode 240 - Step 86246 - Epsilon 0.01 - Mean Reward 1483.0\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_240.pth'\n",
      "Episode 250 - Step 90280 - Epsilon 0.01 - Mean Reward 1901.3\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_250.pth'\n",
      "Episode 260 - Step 92818 - Epsilon 0.01 - Mean Reward 1528.5\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_260.pth'\n",
      "Episode 270 - Step 97133 - Epsilon 0.01 - Mean Reward 2303.6\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_270.pth'\n",
      "Episode 280 - Step 100484 - Epsilon 0.01 - Mean Reward 1783.0\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_280.pth'\n",
      "Episode 290 - Step 103774 - Epsilon 0.01 - Mean Reward 1990.7\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_290.pth'\n",
      "Episode 300 - Step 107136 - Epsilon 0.01 - Mean Reward 1904.8\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_300.pth'\n",
      "Episode 310 - Step 111598 - Epsilon 0.01 - Mean Reward 2244.0\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_310.pth'\n",
      "Episode 320 - Step 117167 - Epsilon 0.01 - Mean Reward 1952.0\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_320.pth'\n",
      "Episode 330 - Step 120813 - Epsilon 0.01 - Mean Reward 1910.5\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_330.pth'\n",
      "Episode 340 - Step 123549 - Epsilon 0.01 - Mean Reward 1595.1\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_340.pth'\n",
      "Episode 350 - Step 127305 - Epsilon 0.01 - Mean Reward 1754.9\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_350.pth'\n",
      "Episode 360 - Step 130322 - Epsilon 0.01 - Mean Reward 1635.8\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_360.pth'\n",
      "Episode 370 - Step 133878 - Epsilon 0.01 - Mean Reward 1939.9\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_370.pth'\n",
      "Episode 380 - Step 135573 - Epsilon 0.01 - Mean Reward 1016.8\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_380.pth'\n",
      "Episode 390 - Step 140402 - Epsilon 0.01 - Mean Reward 1873.9\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_390.pth'\n",
      "Episode 400 - Step 145495 - Epsilon 0.01 - Mean Reward 2103.6\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_400.pth'\n",
      "Episode 410 - Step 149446 - Epsilon 0.01 - Mean Reward 2371.7\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_410.pth'\n",
      "Episode 420 - Step 152854 - Epsilon 0.01 - Mean Reward 1834.2\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_420.pth'\n",
      "Episode 430 - Step 156451 - Epsilon 0.01 - Mean Reward 2199.9\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_430.pth'\n",
      "Episode 440 - Step 159444 - Epsilon 0.01 - Mean Reward 1599.4\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_440.pth'\n",
      "Episode 450 - Step 163883 - Epsilon 0.01 - Mean Reward 2110.1\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_450.pth'\n",
      "Episode 460 - Step 167261 - Epsilon 0.01 - Mean Reward 1856.9\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_460.pth'\n",
      "Episode 470 - Step 171252 - Epsilon 0.01 - Mean Reward 2075.5\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_470.pth'\n",
      "Episode 480 - Step 174026 - Epsilon 0.01 - Mean Reward 1717.2\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_480.pth'\n",
      "Episode 490 - Step 176448 - Epsilon 0.01 - Mean Reward 1476.5\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_490.pth'\n",
      "Episode 500 - Step 180121 - Epsilon 0.01 - Mean Reward 2106.8\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_500.pth'\n",
      "Episode 510 - Step 183105 - Epsilon 0.01 - Mean Reward 1588.8\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_510.pth'\n",
      "Episode 520 - Step 186697 - Epsilon 0.01 - Mean Reward 2236.3\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_520.pth'\n",
      "Episode 530 - Step 189192 - Epsilon 0.01 - Mean Reward 1484.3\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_530.pth'\n",
      "Episode 540 - Step 192017 - Epsilon 0.01 - Mean Reward 1648.0\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_540.pth'\n",
      "Episode 550 - Step 195548 - Epsilon 0.01 - Mean Reward 1867.0\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_550.pth'\n",
      "Episode 560 - Step 198799 - Epsilon 0.01 - Mean Reward 1787.6\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_560.pth'\n",
      "Episode 570 - Step 201423 - Epsilon 0.01 - Mean Reward 1384.7\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_570.pth'\n",
      "Episode 580 - Step 204611 - Epsilon 0.01 - Mean Reward 1766.7\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_580.pth'\n",
      "Episode 590 - Step 207707 - Epsilon 0.01 - Mean Reward 1752.8\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_590.pth'\n",
      "Episode 600 - Step 210261 - Epsilon 0.01 - Mean Reward 1391.1\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_600.pth'\n",
      "Episode 610 - Step 213308 - Epsilon 0.01 - Mean Reward 1797.7\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_610.pth'\n",
      "Episode 620 - Step 217591 - Epsilon 0.01 - Mean Reward 1421.6\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_620.pth'\n",
      "Episode 630 - Step 220115 - Epsilon 0.01 - Mean Reward 1352.2\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_630.pth'\n",
      "Episode 640 - Step 223158 - Epsilon 0.01 - Mean Reward 1827.8\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_640.pth'\n",
      "Episode 650 - Step 225218 - Epsilon 0.01 - Mean Reward 1254.7\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_650.pth'\n",
      "Episode 660 - Step 227444 - Epsilon 0.01 - Mean Reward 1402.8\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_660.pth'\n",
      "Episode 670 - Step 230506 - Epsilon 0.01 - Mean Reward 1802.1\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_670.pth'\n",
      "Episode 680 - Step 233605 - Epsilon 0.01 - Mean Reward 1773.9\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_680.pth'\n",
      "Episode 690 - Step 237182 - Epsilon 0.01 - Mean Reward 1759.2\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_690.pth'\n",
      "Episode 700 - Step 240334 - Epsilon 0.01 - Mean Reward 1899.6\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_700.pth'\n",
      "Episode 710 - Step 244784 - Epsilon 0.01 - Mean Reward 1690.5\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_710.pth'\n",
      "Episode 720 - Step 246350 - Epsilon 0.01 - Mean Reward 894.9\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_720.pth'\n",
      "Episode 730 - Step 249834 - Epsilon 0.01 - Mean Reward 1598.7\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_730.pth'\n",
      "Episode 740 - Step 252647 - Epsilon 0.01 - Mean Reward 1693.8\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_740.pth'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 750 - Step 256239 - Epsilon 0.01 - Mean Reward 1882.6\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_750.pth'\n",
      "Episode 760 - Step 259649 - Epsilon 0.01 - Mean Reward 1969.0\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_760.pth'\n",
      "Episode 770 - Step 262589 - Epsilon 0.01 - Mean Reward 1576.5\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_770.pth'\n",
      "Episode 780 - Step 267416 - Epsilon 0.01 - Mean Reward 1884.9\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_780.pth'\n",
      "Episode 790 - Step 270768 - Epsilon 0.01 - Mean Reward 2056.4\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_790.pth'\n",
      "Episode 800 - Step 276207 - Epsilon 0.01 - Mean Reward 2492.6\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_800.pth'\n",
      "Episode 810 - Step 279985 - Epsilon 0.01 - Mean Reward 1512.2\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_810.pth'\n",
      "Episode 820 - Step 286090 - Epsilon 0.01 - Mean Reward 2721.4\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_820.pth'\n",
      "Episode 830 - Step 290370 - Epsilon 0.01 - Mean Reward 2540.7\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_830.pth'\n",
      "Episode 840 - Step 294579 - Epsilon 0.01 - Mean Reward 2192.1\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_840.pth'\n",
      "Episode 850 - Step 297779 - Epsilon 0.01 - Mean Reward 2042.3\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_850.pth'\n",
      "Episode 860 - Step 300872 - Epsilon 0.01 - Mean Reward 1830.1\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_860.pth'\n",
      "Episode 870 - Step 305011 - Epsilon 0.01 - Mean Reward 1640.5\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_870.pth'\n",
      "Episode 880 - Step 309151 - Epsilon 0.01 - Mean Reward 1609.0\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_880.pth'\n",
      "Episode 890 - Step 312094 - Epsilon 0.01 - Mean Reward 1696.9\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_890.pth'\n",
      "Episode 900 - Step 314662 - Epsilon 0.01 - Mean Reward 1503.6\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_900.pth'\n",
      "Episode 910 - Step 317886 - Epsilon 0.01 - Mean Reward 1865.5\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_910.pth'\n",
      "Episode 920 - Step 320937 - Epsilon 0.01 - Mean Reward 1437.5\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_920.pth'\n",
      "Episode 930 - Step 324444 - Epsilon 0.01 - Mean Reward 2114.5\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_930.pth'\n",
      "Episode 940 - Step 329944 - Epsilon 0.01 - Mean Reward 1907.4\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_940.pth'\n",
      "Episode 950 - Step 333046 - Epsilon 0.01 - Mean Reward 1644.0\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_950.pth'\n",
      "Episode 960 - Step 337225 - Epsilon 0.01 - Mean Reward 2023.9\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_960.pth'\n",
      "Episode 970 - Step 340010 - Epsilon 0.01 - Mean Reward 1671.3\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_970.pth'\n",
      "Episode 980 - Step 341769 - Epsilon 0.01 - Mean Reward 1128.6\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_980.pth'\n",
      "Episode 990 - Step 342760 - Epsilon 0.01 - Mean Reward 604.9\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_990.pth'\n",
      "Episode 1000 - Step 346051 - Epsilon 0.01 - Mean Reward 1363.9\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_1000.pth'\n",
      "Episode 1010 - Step 348078 - Epsilon 0.01 - Mean Reward 1292.6\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_1010.pth'\n",
      "Episode 1020 - Step 351570 - Epsilon 0.01 - Mean Reward 1789.3\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_1020.pth'\n",
      "Episode 1030 - Step 353449 - Epsilon 0.01 - Mean Reward 1147.3\n",
      "Checkpoint saved to 'mario_ql\\checkpoint_1030.pth'\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import os\n",
    "import random\n",
    "from collections import deque\n",
    "import gym\n",
    "import gym_super_mario_bros\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from gym.spaces import Box\n",
    "from gym.wrappers import FrameStack\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "class SkipFrame(gym.Wrapper):\n",
    "    def __init__(self, env, skip):\n",
    "        super().__init__(env)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        total_reward = 0.0\n",
    "        done = False\n",
    "        for i in range(self._skip):\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        return obs, total_reward, done, info\n",
    "\n",
    "\n",
    "class GrayScaleObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.observation_space = Box(low=0, high=255, shape=self.observation_space.shape[:2], dtype=np.uint8)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        transform = transforms.Grayscale()\n",
    "        return transform(torch.tensor(np.transpose(observation, (2, 0, 1)).copy(), dtype=torch.float))\n",
    "\n",
    "\n",
    "class ResizeObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env, shape):\n",
    "        super().__init__(env)\n",
    "        self.shape = (shape, shape)\n",
    "        obs_shape = self.shape + self.observation_space.shape[2:]\n",
    "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        transformations = transforms.Compose([transforms.Resize(self.shape), transforms.Normalize(0, 255)])\n",
    "        return transformations(observation).squeeze(0)\n",
    "\n",
    "\n",
    "env = gym_super_mario_bros.make('SuperMarioBros-1-1-v0')\n",
    "env = JoypadSpace(env, [[\"right\"], [\"right\", \"A\"]])\n",
    "env = FrameStack(ResizeObservation(GrayScaleObservation(SkipFrame(env, skip=4)), shape=84), num_stack=4)\n",
    "\n",
    "\n",
    "\n",
    "class DDQNSolver(nn.Module):\n",
    "    def __init__(self, output_dim):\n",
    "        super().__init__()\n",
    "        self.online = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=4, out_channels=32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(3136, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, output_dim),\n",
    "        )\n",
    "        self.target = copy.deepcopy(self.online)\n",
    "        for p in self.target.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    def forward(self, input, model):\n",
    "        if model == \"online\":\n",
    "            return self.online(input)\n",
    "        elif model == \"target\":\n",
    "            return self.target(input)\n",
    "\n",
    "\n",
    "class DDQNAgent:\n",
    "    def __init__(self, action_dim, save_directory):\n",
    "        self.action_dim = action_dim\n",
    "        self.save_directory = save_directory\n",
    "        self.net = DDQNSolver(self.action_dim).cuda()\n",
    "        self.exploration_rate = 1.0\n",
    "        self.exploration_rate_decay = 0.99\n",
    "        self.exploration_rate_min = 0.01\n",
    "        self.current_step = 0\n",
    "        self.memory = deque(maxlen=100000)\n",
    "        self.batch_size = 32\n",
    "        self.gamma = 0.95\n",
    "        self.sync_period = 1e4\n",
    "        self.optimizer = torch.optim.Adam(self.net.parameters(), lr=0.00025, eps=1e-4)\n",
    "        self.loss = torch.nn.SmoothL1Loss()\n",
    "        self.episode_rewards = []\n",
    "        self.moving_average_episode_rewards = []\n",
    "        self.current_episode_reward = 0.0\n",
    "\n",
    "    def log_episode(self):\n",
    "        self.episode_rewards.append(self.current_episode_reward)\n",
    "        self.current_episode_reward = 0.0\n",
    "\n",
    "    def log_period(self, episode, epsilon, step):\n",
    "        self.moving_average_episode_rewards.append(np.round(np.mean(self.episode_rewards[-checkpoint_period:]), 3))\n",
    "        print(f\"Episode {episode} - Step {step} - Epsilon {epsilon} - Mean Reward {self.moving_average_episode_rewards[-1]}\")\n",
    "        plt.plot(self.moving_average_episode_rewards)\n",
    "        plt.savefig(os.path.join(self.save_directory, f\"episode_rewards_plot_{episode}.png\"))\n",
    "        plt.clf()\n",
    "\n",
    "    def remember(self, state, next_state, action, reward, done):\n",
    "        self.memory.append((torch.tensor(state.__array__()), torch.tensor(next_state.__array__()),\n",
    "                            torch.tensor([action]), torch.tensor([reward]), torch.tensor([done])))\n",
    "\n",
    "    def experience_replay(self, step_reward):\n",
    "        self.current_episode_reward += step_reward\n",
    "        if self.current_step % self.sync_period == 0:\n",
    "            self.net.target.load_state_dict(self.net.online.state_dict())\n",
    "        if self.batch_size > len(self.memory):\n",
    "            return\n",
    "        state, next_state, action, reward, done = self.recall()\n",
    "        q_estimate = self.net(state.cuda(), model=\"online\")[np.arange(0, self.batch_size), action.cuda()]\n",
    "        with torch.no_grad():\n",
    "            best_action = torch.argmax(self.net(next_state.cuda(), model=\"online\"), dim=1)\n",
    "            next_q = self.net(next_state.cuda(), model=\"target\")[np.arange(0, self.batch_size), best_action]\n",
    "            q_target = (reward.cuda() + (1 - done.cuda().float()) * self.gamma * next_q).float()\n",
    "        loss = self.loss(q_estimate, q_target)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def recall(self):\n",
    "        state, next_state, action, reward, done = map(torch.stack, zip(*random.sample(self.memory, self.batch_size)))\n",
    "        return state, next_state, action.squeeze(), reward.squeeze(), done.squeeze()\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() < self.exploration_rate:\n",
    "            action = np.random.randint(self.action_dim)\n",
    "        else:\n",
    "            action_values = self.net(torch.tensor(state.__array__()).cuda().unsqueeze(0), model=\"online\")\n",
    "            action = torch.argmax(action_values, dim=1).item()\n",
    "        self.exploration_rate *= self.exploration_rate_decay\n",
    "        self.exploration_rate = max(self.exploration_rate_min, self.exploration_rate)\n",
    "        self.current_step += 1\n",
    "        return action\n",
    "\n",
    "    def load_checkpoint(self, path):\n",
    "        checkpoint = torch.load(path)\n",
    "        self.net.load_state_dict(checkpoint['model'])\n",
    "        self.exploration_rate = checkpoint['exploration_rate']\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        filename = os.path.join(self.save_directory, 'checkpoint_{}.pth'.format(episode))\n",
    "        torch.save(dict(model=self.net.state_dict(), exploration_rate=self.exploration_rate), f=filename)\n",
    "        print('Checkpoint saved to \\'{}\\''.format(filename))\n",
    "\n",
    "\n",
    "checkpoint_period = 10\n",
    "save_directory = \"mario_ql\"\n",
    "load_checkpoint =\"checkpoint_9710.pth\"\n",
    "agent = DDQNAgent(action_dim=env.action_space.n, save_directory=save_directory)\n",
    "if load_checkpoint is not None:\n",
    "    agent.load_checkpoint(save_directory + \"/\" + load_checkpoint)\n",
    "episode = 0\n",
    "while True:\n",
    "    state = env.reset()\n",
    "    while True:\n",
    "        action = agent.act(state)\n",
    "        env.render()\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        agent.remember(state, next_state, action, reward, done)\n",
    "        agent.experience_replay(reward)\n",
    "        state = next_state\n",
    "        if done:\n",
    "            episode += 1\n",
    "            agent.log_episode()\n",
    "            if episode % checkpoint_period == 0:\n",
    "                agent.log_period(episode=episode, epsilon=agent.exploration_rate, step=agent.current_step)\n",
    "                agent.save_checkpoint()\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2010205",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ade89441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 1, average reward: 766.1\n",
      "Batch: 2, average reward: 438.3\n",
      "Batch: 3, average reward: 748.3\n",
      "Batch: 4, average reward: 787.1\n",
      "Batch: 5, average reward: 725.7\n",
      "Checkpoint saved to './MARIO_DQN\\checkpoint_50.pth'\n",
      "Batch: 6, average reward: 730.3\n",
      "Batch: 7, average reward: 556.0\n",
      "Batch: 8, average reward: 711.6\n",
      "Batch: 9, average reward: 591.9\n",
      "Batch: 10, average reward: 648.4\n",
      "Checkpoint saved to './MARIO_DQN\\checkpoint_100.pth'\n",
      "Batch: 11, average reward: 903.8\n",
      "Batch: 12, average reward: 600.7\n",
      "Batch: 13, average reward: 705.5\n",
      "Batch: 14, average reward: 570.4\n",
      "Batch: 15, average reward: 596.2\n",
      "Checkpoint saved to './MARIO_DQN\\checkpoint_150.pth'\n",
      "Batch: 16, average reward: 559.9\n",
      "Batch: 17, average reward: 904.7\n",
      "Batch: 18, average reward: 754.7\n",
      "Batch: 19, average reward: 828.6\n",
      "Batch: 20, average reward: 430.6\n",
      "Checkpoint saved to './MARIO_DQN\\checkpoint_200.pth'\n",
      "Batch: 21, average reward: 508.3\n",
      "Batch: 22, average reward: 626.3\n",
      "Batch: 23, average reward: 528.3\n",
      "Batch: 24, average reward: 670.4\n",
      "Batch: 25, average reward: 756.0\n",
      "Checkpoint saved to './MARIO_DQN\\checkpoint_250.pth'\n",
      "Batch: 26, average reward: 604.4\n",
      "Batch: 27, average reward: 562.0\n",
      "Batch: 28, average reward: 805.6\n",
      "Batch: 29, average reward: 678.6\n",
      "Batch: 30, average reward: 709.5\n",
      "Checkpoint saved to './MARIO_DQN\\checkpoint_300.pth'\n",
      "Batch: 31, average reward: 817.8\n",
      "Batch: 32, average reward: 627.0\n",
      "Batch: 33, average reward: 667.3\n",
      "Batch: 34, average reward: 702.7\n",
      "Batch: 35, average reward: 810.6\n",
      "Checkpoint saved to './MARIO_DQN\\checkpoint_350.pth'\n",
      "Batch: 36, average reward: 711.3\n",
      "Batch: 37, average reward: 611.1\n",
      "Batch: 38, average reward: 600.8\n",
      "Batch: 39, average reward: 579.0\n",
      "Batch: 40, average reward: 425.6\n",
      "Checkpoint saved to './MARIO_DQN\\checkpoint_400.pth'\n",
      "Batch: 41, average reward: 720.9\n",
      "Batch: 42, average reward: 478.2\n",
      "Batch: 43, average reward: 665.9\n",
      "Batch: 44, average reward: 628.4\n",
      "Batch: 45, average reward: 699.8\n",
      "Checkpoint saved to './MARIO_DQN\\checkpoint_450.pth'\n",
      "Batch: 46, average reward: 953.9\n",
      "Batch: 47, average reward: 658.4\n",
      "Batch: 48, average reward: 604.3\n",
      "Batch: 49, average reward: 660.6\n",
      "Batch: 50, average reward: 631.7\n",
      "Checkpoint saved to './MARIO_DQN\\checkpoint_500.pth'\n",
      "Batch: 51, average reward: 745.9\n",
      "Batch: 52, average reward: 903.9\n",
      "Batch: 53, average reward: 715.9\n",
      "Batch: 54, average reward: 513.1\n",
      "Batch: 55, average reward: 624.1\n",
      "Checkpoint saved to './MARIO_DQN\\checkpoint_550.pth'\n",
      "Batch: 56, average reward: 819.7\n",
      "Batch: 57, average reward: 808.4\n",
      "Batch: 58, average reward: 716.9\n",
      "Batch: 59, average reward: 520.9\n",
      "Batch: 60, average reward: 304.3\n",
      "Checkpoint saved to './MARIO_DQN\\checkpoint_600.pth'\n",
      "Batch: 61, average reward: 661.7\n",
      "Batch: 62, average reward: 862.3\n",
      "Batch: 63, average reward: 891.8\n",
      "Batch: 64, average reward: 702.2\n",
      "Batch: 65, average reward: 1114.7\n",
      "Checkpoint saved to './MARIO_DQN\\checkpoint_650.pth'\n",
      "Batch: 66, average reward: 641.8\n",
      "Batch: 67, average reward: 675.3\n",
      "Batch: 68, average reward: 635.4\n",
      "Batch: 69, average reward: 608.2\n",
      "Batch: 70, average reward: 523.6\n",
      "Checkpoint saved to './MARIO_DQN\\checkpoint_700.pth'\n",
      "Batch: 71, average reward: 498.0\n",
      "Batch: 72, average reward: 574.5\n",
      "Batch: 73, average reward: 675.7\n",
      "Batch: 74, average reward: 680.0\n",
      "Batch: 75, average reward: 676.8\n",
      "Checkpoint saved to './MARIO_DQN\\checkpoint_750.pth'\n",
      "Batch: 76, average reward: 475.9\n",
      "Batch: 77, average reward: 652.5\n",
      "Batch: 78, average reward: 761.6\n",
      "Batch: 79, average reward: 720.4\n",
      "Batch: 80, average reward: 698.8\n",
      "Checkpoint saved to './MARIO_DQN\\checkpoint_800.pth'\n",
      "Batch: 81, average reward: 709.1\n",
      "Batch: 82, average reward: 706.9\n",
      "Batch: 83, average reward: 812.8\n",
      "Batch: 84, average reward: 780.6\n",
      "Batch: 85, average reward: 703.4\n",
      "Checkpoint saved to './MARIO_DQN\\checkpoint_850.pth'\n",
      "Batch: 86, average reward: 584.3\n",
      "Batch: 87, average reward: 825.8\n",
      "Batch: 88, average reward: 539.4\n",
      "Batch: 89, average reward: 782.5\n",
      "Batch: 90, average reward: 787.7\n",
      "Checkpoint saved to './MARIO_DQN\\checkpoint_900.pth'\n",
      "Batch: 91, average reward: 717.9\n",
      "Batch: 92, average reward: 623.6\n",
      "Batch: 93, average reward: 789.9\n",
      "Batch: 94, average reward: 725.0\n",
      "Batch: 95, average reward: 611.2\n",
      "Checkpoint saved to './MARIO_DQN\\checkpoint_950.pth'\n",
      "Batch: 96, average reward: 528.9\n",
      "Batch: 97, average reward: 837.0\n",
      "Batch: 98, average reward: 512.5\n",
      "Batch: 99, average reward: 653.4\n",
      "Batch: 100, average reward: 642.9\n",
      "Checkpoint saved to './MARIO_DQN\\checkpoint_1000.pth'\n",
      "Batch: 101, average reward: 636.0\n",
      "Batch: 102, average reward: 631.8\n",
      "Batch: 103, average reward: 645.5\n",
      "Batch: 104, average reward: 651.1\n",
      "Batch: 105, average reward: 791.5\n",
      "Checkpoint saved to './MARIO_DQN\\checkpoint_1050.pth'\n",
      "Batch: 106, average reward: 727.2\n",
      "Batch: 107, average reward: 605.7\n",
      "Batch: 108, average reward: 618.5\n",
      "Batch: 109, average reward: 861.0\n",
      "Batch: 110, average reward: 738.3\n",
      "Checkpoint saved to './MARIO_DQN\\checkpoint_1100.pth'\n",
      "Batch: 111, average reward: 808.3\n",
      "Batch: 112, average reward: 828.3\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-bd0955593957>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    133\u001b[0m         \u001b[0mdistribution\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCategorical\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdistribution\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 135\u001b[1;33m         \u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    136\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepisode_actions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepisode_actions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdistribution\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepisode_rewards\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gym\\wrappers\\frame_stack.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 115\u001b[1;33m         \u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    116\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gym\\core.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    321\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    322\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 323\u001b[1;33m         \u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    324\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gym\\core.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    322\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m         \u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 324\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    325\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    326\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-bd0955593957>\u001b[0m in \u001b[0;36mobservation\u001b[1;34m(self, observation)\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mobservation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobservation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[0mtransform\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGrayscale\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torchvision\\transforms\\transforms.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m   1494\u001b[0m             \u001b[0mPIL\u001b[0m \u001b[0mImage\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mGrayscaled\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1495\u001b[0m         \"\"\"\n\u001b[1;32m-> 1496\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrgb_to_grayscale\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_output_channels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_output_channels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1497\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1498\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torchvision\\transforms\\functional.py\u001b[0m in \u001b[0;36mrgb_to_grayscale\u001b[1;34m(img, num_output_channels)\u001b[0m\n\u001b[0;32m   1151\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mF_pil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_grayscale\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_output_channels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1152\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1153\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mF_t\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrgb_to_grayscale\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_output_channels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1154\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1155\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torchvision\\transforms\\functional_tensor.py\u001b[0m in \u001b[0;36mrgb_to_grayscale\u001b[1;34m(img, num_output_channels)\u001b[0m\n\u001b[0;32m    144\u001b[0m     \u001b[1;31m# This implementation closely follows the TF one:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m     \u001b[1;31m# https://github.com/tensorflow/tensorflow/blob/v2.3.0/tensorflow/python/ops/image_ops_impl.py#L2105-L2138\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 146\u001b[1;33m     \u001b[0ml_img\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m0.2989\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mr\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m0.587\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mg\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m0.114\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    147\u001b[0m     \u001b[0ml_img\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ml_img\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from gym.wrappers import FrameStack\n",
    "from torchvision import transforms\n",
    "import gym_super_mario_bros\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "from torch.distributions import Categorical\n",
    "from gym.spaces import Box\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "\n",
    "\n",
    "class SkipFrame(gym.Wrapper):\n",
    "    def __init__(self, env, skip):\n",
    "        super().__init__(env)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        total_reward = 0.0\n",
    "        done = False\n",
    "        for i in range(self._skip):\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        return obs, total_reward, done, info\n",
    "\n",
    "\n",
    "class GrayScaleObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.observation_space = Box(low=0, high=255, shape=self.observation_space.shape[:2], dtype=np.uint8)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        transform = transforms.Grayscale()\n",
    "        return transform(torch.tensor(np.transpose(observation, (2, 0, 1)).copy(), dtype=torch.float))\n",
    "\n",
    "\n",
    "class ResizeObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env, shape):\n",
    "        super().__init__(env)\n",
    "        self.shape = (shape, shape)\n",
    "        obs_shape = self.shape + self.observation_space.shape[2:]\n",
    "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        transformations = transforms.Compose([transforms.Resize(self.shape), transforms.Normalize(0, 255)])\n",
    "        return transformations(observation).squeeze(0)\n",
    "\n",
    "\n",
    "env = gym_super_mario_bros.make('SuperMarioBros-1-1-v0')\n",
    "env = JoypadSpace(env, [[\"right\"], [\"right\", \"A\"]])\n",
    "env = FrameStack(ResizeObservation(GrayScaleObservation(SkipFrame(env, skip=4)), shape=84), num_stack=4)\n",
    "env.seed(42)\n",
    "env.action_space.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.random.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "class MarioSolver:\n",
    "    def __init__(self, learning_rate):\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=4, out_channels=32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(3136, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, env.action_space.n),\n",
    "            nn.Softmax(dim=-1)\n",
    "        ).cuda()\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=learning_rate, eps=1e-4)\n",
    "        self.reset()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def reset(self):\n",
    "        self.episode_actions = torch.tensor([], requires_grad=True).cuda()\n",
    "        self.episode_rewards = []\n",
    "\n",
    "    def save_checkpoint(self, directory, episode):\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "        filename = os.path.join(directory, 'checkpoint_{}.pth'.format(episode))\n",
    "        torch.save(self.model.state_dict(), f=filename)\n",
    "        print('Checkpoint saved to \\'{}\\''.format(filename))\n",
    "\n",
    "    def load_checkpoint(self, directory, filename):\n",
    "        self.model.load_state_dict(torch.load(os.path.join(directory, filename)))\n",
    "        print('Resuming training from checkpoint \\'{}\\'.'.format(filename))\n",
    "        return int(filename[11:-4])\n",
    "\n",
    "    def backward(self):\n",
    "        future_reward = 0\n",
    "        rewards = []\n",
    "        for r in self.episode_rewards[::-1]:\n",
    "            future_reward = r + gamma * future_reward\n",
    "            rewards.append(future_reward)\n",
    "        rewards = torch.tensor(rewards[::-1], dtype=torch.float32).cuda()\n",
    "        rewards = (rewards - rewards.mean()) / (rewards.std() + np.finfo(np.float32).eps)\n",
    "        loss = torch.sum(torch.mul(self.episode_actions, rewards).mul(-1))\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.reset()\n",
    "\n",
    "\n",
    "batch_size = 10\n",
    "gamma = 0.95\n",
    "load_filename = None\n",
    "save_directory = \"./MARIO_DQN\"\n",
    "batch_rewards = []\n",
    "episode = 0\n",
    "\n",
    "model = MarioSolver(learning_rate=0.00025)\n",
    "if load_filename is not None:\n",
    "    episode = model.load_checkpoint(save_directory, load_filename)\n",
    "all_episode_rewards = []\n",
    "all_mean_rewards = []\n",
    "while True:\n",
    "    observation = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        env.render()\n",
    "        observation = torch.tensor(observation.__array__()).cuda().unsqueeze(0)\n",
    "        distribution = Categorical(model.forward(observation))\n",
    "        action = distribution.sample()\n",
    "        observation, reward, done, _ = env.step(action.item())\n",
    "        model.episode_actions = torch.cat([model.episode_actions, distribution.log_prob(action).reshape(1)])\n",
    "        model.episode_rewards.append(reward)\n",
    "        if done:\n",
    "            all_episode_rewards.append(np.sum(model.episode_rewards))\n",
    "            batch_rewards.append(np.sum(model.episode_rewards))\n",
    "            model.backward()\n",
    "            episode += 1\n",
    "            if episode % batch_size == 0:\n",
    "                print('Batch: {}, average reward: {}'.format(episode // batch_size, np.array(batch_rewards).mean()))\n",
    "                batch_rewards = []\n",
    "                all_mean_rewards.append(np.mean(all_episode_rewards[-batch_size:]))\n",
    "                plt.plot(all_mean_rewards)\n",
    "                plt.savefig(\"{}/mean_reward_{}.png\".format(save_directory, episode))\n",
    "                plt.clf()\n",
    "            if episode % 50 == 0 and save_directory is not None:\n",
    "                model.save_checkpoint(save_directory, episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85ad5a05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 10, average reward: 667.5\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_10.pth'\n",
      "Episode: 20, average reward: 788.9000244140625\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_20.pth'\n",
      "Episode: 30, average reward: 585.5\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_30.pth'\n",
      "Episode: 40, average reward: 560.0999755859375\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_40.pth'\n",
      "Episode: 50, average reward: 517.2000122070312\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_50.pth'\n",
      "Episode: 60, average reward: 863.5\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_60.pth'\n",
      "Episode: 70, average reward: 682.5999755859375\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_70.pth'\n",
      "Episode: 80, average reward: 479.3999938964844\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_80.pth'\n",
      "Episode: 90, average reward: 743.4000244140625\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_90.pth'\n",
      "Episode: 100, average reward: 634.7999877929688\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_100.pth'\n",
      "Episode: 110, average reward: 694.7000122070312\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_110.pth'\n",
      "Episode: 120, average reward: 550.0\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_120.pth'\n",
      "Episode: 130, average reward: 828.5999755859375\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_130.pth'\n",
      "Episode: 140, average reward: 981.2000122070312\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_140.pth'\n",
      "Episode: 150, average reward: 706.9000244140625\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_150.pth'\n",
      "Episode: 160, average reward: 858.4000244140625\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_160.pth'\n",
      "Episode: 170, average reward: 718.2999877929688\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_170.pth'\n",
      "Episode: 180, average reward: 744.7000122070312\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_180.pth'\n",
      "Episode: 190, average reward: 476.0\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_190.pth'\n",
      "Episode: 200, average reward: 621.7999877929688\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_200.pth'\n",
      "Episode: 210, average reward: 514.9000244140625\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_210.pth'\n",
      "Episode: 220, average reward: 892.0\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_220.pth'\n",
      "Episode: 230, average reward: 829.0999755859375\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_230.pth'\n",
      "Episode: 240, average reward: 771.2999877929688\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_240.pth'\n",
      "Episode: 250, average reward: 627.0\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_250.pth'\n",
      "Episode: 260, average reward: 767.0\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_260.pth'\n",
      "Episode: 270, average reward: 586.2000122070312\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_270.pth'\n",
      "Episode: 280, average reward: 775.2000122070312\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_280.pth'\n",
      "Episode: 290, average reward: 506.70001220703125\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_290.pth'\n",
      "Episode: 300, average reward: 558.9000244140625\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_300.pth'\n",
      "Episode: 310, average reward: 505.29998779296875\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_310.pth'\n",
      "Episode: 320, average reward: 430.5\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_320.pth'\n",
      "Episode: 330, average reward: 595.9000244140625\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_330.pth'\n",
      "Episode: 340, average reward: 617.0\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_340.pth'\n",
      "Episode: 350, average reward: 712.2000122070312\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_350.pth'\n",
      "Episode: 360, average reward: 979.2000122070312\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_360.pth'\n",
      "Episode: 370, average reward: 624.2000122070312\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_370.pth'\n",
      "Episode: 380, average reward: 709.2000122070312\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_380.pth'\n",
      "Episode: 390, average reward: 836.2999877929688\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_390.pth'\n",
      "Episode: 400, average reward: 710.2000122070312\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_400.pth'\n",
      "Episode: 410, average reward: 595.0999755859375\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_410.pth'\n",
      "Episode: 420, average reward: 948.0999755859375\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_420.pth'\n",
      "Episode: 430, average reward: 885.2000122070312\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_430.pth'\n",
      "Episode: 440, average reward: 809.7000122070312\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_440.pth'\n",
      "Episode: 450, average reward: 632.2000122070312\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_450.pth'\n",
      "Episode: 460, average reward: 709.7000122070312\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_460.pth'\n",
      "Episode: 470, average reward: 958.9000244140625\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_470.pth'\n",
      "Episode: 480, average reward: 946.5\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_480.pth'\n",
      "Episode: 490, average reward: 882.0\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_490.pth'\n",
      "Episode: 500, average reward: 867.7000122070312\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_500.pth'\n",
      "Episode: 510, average reward: 919.9000244140625\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_510.pth'\n",
      "Episode: 520, average reward: 949.0\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_520.pth'\n",
      "Episode: 530, average reward: 904.4000244140625\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_530.pth'\n",
      "Episode: 540, average reward: 1001.4000244140625\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_540.pth'\n",
      "Episode: 550, average reward: 837.0\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_550.pth'\n",
      "Episode: 560, average reward: 867.7000122070312\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_560.pth'\n",
      "Episode: 570, average reward: 766.5999755859375\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_570.pth'\n",
      "Episode: 580, average reward: 884.4000244140625\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_580.pth'\n",
      "Episode: 590, average reward: 843.4000244140625\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_590.pth'\n",
      "Episode: 600, average reward: 703.0\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_600.pth'\n",
      "Episode: 610, average reward: 1062.9000244140625\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_610.pth'\n",
      "Episode: 620, average reward: 968.7999877929688\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_620.pth'\n",
      "Episode: 630, average reward: 1075.800048828125\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_630.pth'\n",
      "Episode: 640, average reward: 944.7000122070312\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_640.pth'\n",
      "Episode: 650, average reward: 532.0\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_650.pth'\n",
      "Episode: 660, average reward: 892.5\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_660.pth'\n",
      "Episode: 670, average reward: 935.0999755859375\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_670.pth'\n",
      "Episode: 680, average reward: 880.7000122070312\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_680.pth'\n",
      "Episode: 690, average reward: 931.7999877929688\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_690.pth'\n",
      "Episode: 700, average reward: 923.4000244140625\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_700.pth'\n",
      "Episode: 710, average reward: 844.5999755859375\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_710.pth'\n",
      "Episode: 720, average reward: 773.2999877929688\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_720.pth'\n",
      "Episode: 730, average reward: 838.0\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_730.pth'\n",
      "Episode: 740, average reward: 979.2000122070312\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_740.pth'\n",
      "Episode: 750, average reward: 673.4000244140625\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_750.pth'\n",
      "Episode: 760, average reward: 970.4000244140625\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_760.pth'\n",
      "Episode: 770, average reward: 736.9000244140625\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_770.pth'\n",
      "Episode: 780, average reward: 701.0\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_780.pth'\n",
      "Episode: 790, average reward: 738.9000244140625\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_790.pth'\n",
      "Episode: 800, average reward: 869.9000244140625\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_800.pth'\n",
      "Episode: 810, average reward: 546.0\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_810.pth'\n",
      "Episode: 820, average reward: 962.4000244140625\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_820.pth'\n",
      "Episode: 830, average reward: 848.0\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_830.pth'\n",
      "Episode: 840, average reward: 942.7999877929688\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_840.pth'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 850, average reward: 862.9000244140625\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_850.pth'\n",
      "Episode: 860, average reward: 594.9000244140625\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_860.pth'\n",
      "Episode: 870, average reward: 605.2000122070312\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_870.pth'\n",
      "Episode: 880, average reward: 711.5999755859375\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_880.pth'\n",
      "Episode: 890, average reward: 811.0999755859375\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_890.pth'\n",
      "Episode: 900, average reward: 1017.0\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_900.pth'\n",
      "Episode: 910, average reward: 841.0\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_910.pth'\n",
      "Episode: 920, average reward: 876.2000122070312\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_920.pth'\n",
      "Episode: 930, average reward: 976.7999877929688\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_930.pth'\n",
      "Episode: 940, average reward: 967.2999877929688\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_940.pth'\n",
      "Episode: 950, average reward: 705.7000122070312\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_950.pth'\n",
      "Episode: 960, average reward: 838.2000122070312\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_960.pth'\n",
      "Episode: 970, average reward: 923.2999877929688\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_970.pth'\n",
      "Episode: 980, average reward: 1062.5\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_980.pth'\n",
      "Episode: 990, average reward: 781.0999755859375\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_990.pth'\n",
      "Episode: 1000, average reward: 891.4000244140625\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_1000.pth'\n",
      "Episode: 1010, average reward: 1019.5999755859375\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_1010.pth'\n",
      "Episode: 1020, average reward: 883.4000244140625\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_1020.pth'\n",
      "Episode: 1030, average reward: 880.5999755859375\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_1030.pth'\n",
      "Episode: 1040, average reward: 882.7999877929688\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_1040.pth'\n",
      "Episode: 1050, average reward: 1102.4000244140625\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_1050.pth'\n",
      "Episode: 1060, average reward: 1012.5\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_1060.pth'\n",
      "Episode: 1070, average reward: 1046.9000244140625\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_1070.pth'\n",
      "Episode: 1080, average reward: 637.0999755859375\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_1080.pth'\n",
      "Episode: 1090, average reward: 1034.800048828125\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_1090.pth'\n",
      "Episode: 1100, average reward: 1186.0999755859375\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_1100.pth'\n",
      "Episode: 1110, average reward: 1058.199951171875\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_1110.pth'\n",
      "Episode: 1120, average reward: 1010.5\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_1120.pth'\n",
      "Episode: 1130, average reward: 1036.800048828125\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_1130.pth'\n",
      "Episode: 1140, average reward: 915.7000122070312\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_1140.pth'\n",
      "Episode: 1150, average reward: 1046.5\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_1150.pth'\n",
      "Episode: 1160, average reward: 972.4000244140625\n",
      "Checkpoint saved to './mario_ppo\\checkpoint_1160.pth'\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-e152f69058bc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[0msolver\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPPOSolver\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 216\u001b[1;33m     \u001b[0msolver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msolver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-1-e152f69058bc>\u001b[0m in \u001b[0;36msample\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    144\u001b[0m                 \u001b[0mactions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m                 \u001b[0mlog_pis\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 146\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    147\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__array__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m             \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gym\\wrappers\\frame_stack.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 115\u001b[1;33m         \u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    116\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gym\\core.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    321\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    322\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 323\u001b[1;33m         \u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    324\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gym\\core.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    321\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    322\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 323\u001b[1;33m         \u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    324\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-e152f69058bc>\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mdone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_skip\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m             \u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m             \u001b[0mtotal_reward\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nes_py\\wrappers\\joypad_space.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     72\u001b[0m         \"\"\"\n\u001b[0;32m     73\u001b[0m         \u001b[1;31m# take the step and record the output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_action_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gym\\wrappers\\time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     16\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         ), \"Cannot call env.step() before calling reset()\"\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_max_episode_steps\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nes_py\\nes_env.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    291\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrollers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    292\u001b[0m         \u001b[1;31m# pass the action to the emulator as an unsigned byte\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 293\u001b[1;33m         \u001b[0m_LIB\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_env\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    294\u001b[0m         \u001b[1;31m# get the reward for this step\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m         \u001b[0mreward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_reward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from torchvision import transforms\n",
    "import gym\n",
    "from gym.spaces import Box\n",
    "import gym_super_mario_bros\n",
    "import numpy as np\n",
    "import torch\n",
    "from gym.wrappers import FrameStack\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "from torch import nn\n",
    "from torch.distributions import Categorical\n",
    "import matplotlib.pyplot as plt\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "\n",
    "class SkipFrame(gym.Wrapper):\n",
    "    def __init__(self, env, skip):\n",
    "        super().__init__(env)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        total_reward = 0.0\n",
    "        done = False\n",
    "        for i in range(self._skip):\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        return obs, total_reward, done, info\n",
    "\n",
    "\n",
    "class GrayScaleObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.observation_space = Box(low=0, high=255, shape=self.observation_space.shape[:2], dtype=np.uint8)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        transform = transforms.Grayscale()\n",
    "        return transform(torch.tensor(np.transpose(observation, (2, 0, 1)).copy(), dtype=torch.float))\n",
    "\n",
    "\n",
    "class ResizeObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env, shape):\n",
    "        super().__init__(env)\n",
    "        self.shape = (shape, shape)\n",
    "        obs_shape = self.shape + self.observation_space.shape[2:]\n",
    "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        transformations = transforms.Compose([transforms.Resize(self.shape), transforms.Normalize(0, 255)])\n",
    "        return transformations(observation).squeeze(0)\n",
    "\n",
    "\n",
    "env = gym_super_mario_bros.make('SuperMarioBros-1-1-v0')\n",
    "env = JoypadSpace(env, [[\"right\"], [\"right\", \"A\"]])\n",
    "env = FrameStack(ResizeObservation(GrayScaleObservation(SkipFrame(env, skip=4)), shape=84), num_stack=4)\n",
    "env.seed(42)\n",
    "env.action_space.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.random.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=4, out_channels=32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(3136, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, env.action_space.n)\n",
    "        )\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=4, out_channels=32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(3136, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, obs):\n",
    "        return Categorical(logits=self.actor(obs)), self.critic(obs).reshape(-1)\n",
    "\n",
    "\n",
    "class PPOSolver:\n",
    "    def __init__(self):\n",
    "        self.rewards = []\n",
    "        self.gamma = 0.95\n",
    "        self.lamda = 0.95\n",
    "        self.worker_steps = 4096\n",
    "        self.n_mini_batch = 4\n",
    "        self.epochs = 30\n",
    "        self.save_directory = \"./mario_ppo\"\n",
    "        self.batch_size = self.worker_steps\n",
    "        self.mini_batch_size = self.batch_size // self.n_mini_batch\n",
    "        self.obs = env.reset().__array__()\n",
    "        self.policy = Model().to(device)\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        self.optimizer = torch.optim.Adam([\n",
    "            {'params': self.policy.actor.parameters(), 'lr': 0.00025},\n",
    "            {'params': self.policy.critic.parameters(), 'lr': 0.001}\n",
    "        ], eps=1e-4)\n",
    "        self.policy_old = Model().to(device)\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "        self.all_episode_rewards = []\n",
    "        self.all_mean_rewards = []\n",
    "        self.episode = 0\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        filename = os.path.join(self.save_directory, 'checkpoint_{}.pth'.format(self.episode))\n",
    "        torch.save(self.policy_old.state_dict(), f=filename)\n",
    "        print('Checkpoint saved to \\'{}\\''.format(filename))\n",
    "\n",
    "    def load_checkpoint(self, filename):\n",
    "        self.policy.load_state_dict(torch.load(os.path.join(self.save_directory, filename)))\n",
    "        self.policy_old.load_state_dict(torch.load(os.path.join(self.save_directory, filename)))\n",
    "        print('Resuming training from checkpoint \\'{}\\'.'.format(filename))\n",
    "\n",
    "    def sample(self):\n",
    "        rewards = np.zeros(self.worker_steps, dtype=np.float32)\n",
    "        actions = np.zeros(self.worker_steps, dtype=np.int32)\n",
    "        done = np.zeros(self.worker_steps, dtype=bool)\n",
    "        obs = np.zeros((self.worker_steps, 4, 84, 84), dtype=np.float32)\n",
    "        log_pis = np.zeros(self.worker_steps, dtype=np.float32)\n",
    "        values = np.zeros(self.worker_steps, dtype=np.float32)\n",
    "        for t in range(self.worker_steps):\n",
    "            with torch.no_grad():\n",
    "                obs[t] = self.obs\n",
    "                pi, v = self.policy_old(torch.tensor(self.obs, dtype=torch.float32, device=device).unsqueeze(0))\n",
    "                values[t] = v.cpu().numpy()\n",
    "                a = pi.sample()\n",
    "                actions[t] = a.cpu().numpy()\n",
    "                log_pis[t] = pi.log_prob(a).cpu().numpy()\n",
    "            self.obs, rewards[t], done[t], _ = env.step(actions[t])\n",
    "            self.obs = self.obs.__array__()\n",
    "            env.render()\n",
    "            self.rewards.append(rewards[t])\n",
    "            if done[t]:\n",
    "                self.episode += 1\n",
    "                self.all_episode_rewards.append(np.sum(self.rewards))\n",
    "                self.rewards = []\n",
    "                env.reset()\n",
    "                if self.episode % 10 == 0:\n",
    "                    print('Episode: {}, average reward: {}'.format(self.episode, np.mean(self.all_episode_rewards[-10:])))\n",
    "                    self.all_mean_rewards.append(np.mean(self.all_episode_rewards[-10:]))\n",
    "                    plt.plot(self.all_mean_rewards)\n",
    "                    plt.savefig(\"{}/mean_reward_{}.png\".format(self.save_directory, self.episode))\n",
    "                    plt.clf()\n",
    "                    self.save_checkpoint()\n",
    "        returns, advantages = self.calculate_advantages(done, rewards, values)\n",
    "        return {\n",
    "            'obs': torch.tensor(obs.reshape(obs.shape[0], *obs.shape[1:]), dtype=torch.float32, device=device),\n",
    "            'actions': torch.tensor(actions, device=device),\n",
    "            'values': torch.tensor(values, device=device),\n",
    "            'log_pis': torch.tensor(log_pis, device=device),\n",
    "            'advantages': torch.tensor(advantages, device=device, dtype=torch.float32),\n",
    "            'returns': torch.tensor(returns, device=device, dtype=torch.float32)\n",
    "        }\n",
    "\n",
    "    def calculate_advantages(self, done, rewards, values):\n",
    "        _, last_value = self.policy_old(torch.tensor(self.obs, dtype=torch.float32, device=device).unsqueeze(0))\n",
    "        last_value = last_value.cpu().data.numpy()\n",
    "        values = np.append(values, last_value)\n",
    "        returns = []\n",
    "        gae = 0\n",
    "        for i in reversed(range(len(rewards))):\n",
    "            mask = 1.0 - done[i]\n",
    "            delta = rewards[i] + self.gamma * values[i + 1] * mask - values[i]\n",
    "            gae = delta + self.gamma * self.lamda * mask * gae\n",
    "            returns.insert(0, gae + values[i])\n",
    "        adv = np.array(returns) - values[:-1]\n",
    "        return returns, (adv - np.mean(adv)) / (np.std(adv) + 1e-8)\n",
    "\n",
    "    def train(self, samples, clip_range):\n",
    "        indexes = torch.randperm(self.batch_size)\n",
    "        for start in range(0, self.batch_size, self.mini_batch_size):\n",
    "            end = start + self.mini_batch_size\n",
    "            mini_batch_indexes = indexes[start: end]\n",
    "            mini_batch = {}\n",
    "            for k, v in samples.items():\n",
    "                mini_batch[k] = v[mini_batch_indexes]\n",
    "            for _ in range(self.epochs):\n",
    "                loss = self.calculate_loss(clip_range=clip_range, samples=mini_batch)\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "            self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "\n",
    "    def calculate_loss(self, samples, clip_range):\n",
    "        sampled_returns = samples['returns']\n",
    "        sampled_advantages = samples['advantages']\n",
    "        pi, value = self.policy(samples['obs'])\n",
    "        ratio = torch.exp(pi.log_prob(samples['actions']) - samples['log_pis'])\n",
    "        clipped_ratio = ratio.clamp(min=1.0 - clip_range, max=1.0 + clip_range)\n",
    "        policy_reward = torch.min(ratio * sampled_advantages, clipped_ratio * sampled_advantages)\n",
    "        entropy_bonus = pi.entropy()\n",
    "        vf_loss = self.mse_loss(value, sampled_returns)\n",
    "        loss = -policy_reward + 0.5 * vf_loss - 0.01 * entropy_bonus\n",
    "        return loss.mean()\n",
    "\n",
    "\n",
    "solver = PPOSolver()\n",
    "while True:\n",
    "    solver.train(solver.sample(), 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4b632e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
